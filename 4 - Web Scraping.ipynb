{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11751109",
   "metadata": {
    "id": "f9ACqHF0wB4q"
   },
   "source": [
    "                                                                              Helga Sigríður Magnúsdóttir s202027 \n",
    "                                                                                 Hlynur Árni Sigurjónsson s192302\n",
    "                                                                             Katrín Erla Bergsveinsdóttir s202026\n",
    "                                                                                Kristín Björk Lilliendahl s192296\n",
    " \n",
    " ![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f881c23d",
   "metadata": {
    "id": "6hPXE6zxwB4x"
   },
   "source": [
    "<img src=\"https://susfans.eu/sites/default/files/clients/DTU.png\"  align=\"right\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d61c8d7",
   "metadata": {
    "id": "e1LHBR2CbUtJ"
   },
   "source": [
    "# Web Scraping  Restaurant Reviews on Tripadvisor\n",
    "\n",
    "### What information do we want and how to retreive it ?\n",
    " \n",
    "The idea is to scrape the reviews of all the restaurants in Copenhagen and their general information, such as cuisine type, rank and location. In addition to that we wanted to get the information about reviewers, with information like how many contributions and followers a reviewer has. The scraping tool is built on python packages such as **BeautifulSoup** and **Selenium**. When designing a scraping tool one has to think carefully about all the specific tags and diffrences that can arise during a scraping run like this. The amount of data in our case is really big and the run time was about 45 hours in total.\n",
    "\n",
    "## Tools and Packages\n",
    "\n",
    "* Beutiful Soup, for HTML extraction.\n",
    "* CSV reader, for writing to csv files.\n",
    "* Selenium Web Driver, for browser loading and actions.\n",
    "\n",
    "## Contents\n",
    "\n",
    "* [1. Scraper Info](#scraper)\n",
    "* [2. Restaurant Info](#restinfo)\n",
    "* [3. Reviews](#reviews)\n",
    "* [4. Reviewer Info](#revinfo)\n",
    "* [5. Improvements](#improv)\n",
    "\n",
    "---\n",
    "\n",
    "### Let's start by importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1234f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import csv \n",
    "from selenium import webdriver\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb9529",
   "metadata": {
    "id": "uvZWAcICwB41"
   },
   "source": [
    " ![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f479974",
   "metadata": {
    "id": "vZC7a55AimlQ"
   },
   "source": [
    "<a id='scraper'></a>\n",
    "# 1. Scraper info "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979f1b2e",
   "metadata": {
    "id": "8b3IoSCnimlR"
   },
   "source": [
    "A scraper tool is created to gather the reviews and the restaurants information into csv files. The skeleton of the tool was taken from a github page [LaskasP](https://github.com/LaskasP/TripAdvisor-Python-Scraper-Restaurants-2021). It later turned out that the code had many errors and crashed after a few calls. So the scraper tool was fixed and improved, with enriching the information gathered. In addition, information about reviewers was created. Since the data was large and the tool was expected to encounter some errors on the way, the urls links are stored into a csv. In our case, we chose to look into resturant in the Copenhagen area, with over 1900 resturants available. Each restaurant has on average 700 reviews so the scraping time is quick to add up.\n",
    "\n",
    "The scraper is based on the **beautifulsoup** package and **selenium**. The reason for using selenium is to open and click on things, to retrieve next pages or additional information.\n",
    "\n",
    "#### Selenium actions:\n",
    "* Click the \"next\" button since Tripadvisor only displays 20 restaurants or reviews at each page\n",
    "* Click the \"boxes\" that pop out with additional information about reviewers\n",
    "* Click on the \"more\" button when a review is exceeding a certain length\n",
    "\n",
    "#### Procedure:\n",
    "* Find a Tripadvisor page with the selected area and select only restaurants\n",
    "* Run the **scrapeRestaurantsUrlsAll** function, this function retrieves all the urls in the selected area\n",
    "* Run through all the urls and scrape the reviews with the **get_reviews function**\n",
    "* If successful retrieval of all reviews, then remove the resturant urls csv file\n",
    "* Sperately run the **scrapeRestaurantInfo** function to get the information of the restaurants\n",
    "\n",
    "As can be seen in the three code snippets below there are a lot of \"try: except:\" clauses in the code. This is due to many smaller deviation on the Tripadvisor webpage. Data can be missing for some restaurants so the scraper tries to retrieve them, if not successful, it is left empty.\n",
    "\n",
    "### Lets create the csv files we write our scraped data to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f961986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToReviews = \"TripReviews.csv\"\n",
    "pathToRestaurantInfo = \"RestaurantInfo.csv\"\n",
    "pathtoReviewers = \"reviewerInfo.csv\"\n",
    "pathAllRestaurants = \"AllRestaurants.txt\" \n",
    "\n",
    "with open(pathToStoreInfo, mode='a', encoding=\"utf-8\") as trip:\n",
    "    data_writer = csv.writer(trip, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "    data_writer.writerow(['storeName', 'storeAddress', 'avgRating', 'nrReviews', 'priceCategory','CousineType', 'Rank'])\n",
    "\n",
    "with open(pathToReviews, mode='a', encoding=\"utf-8\") as trip_data:\n",
    "    data_writer = csv.writer(trip_data, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "    data_writer.writerow(['storeName', 'reviewerUsername', 'ratingDate', 'reviewHeader','reviewText', 'rating'])\n",
    "    \n",
    "with open(pathtoReviewers, mode='a', encoding=\"utf-8\") as reviewer_data:\n",
    "    data_writer = csv.writer(reviewer_data, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "    data_writer.writerow(['username', 'location', 'joined', 'nrContributions','nrReviews', 'nrUpvotes', 'nrFollowers','followers', 'nrFollowing','following'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e196395",
   "metadata": {
    "id": "kznKkmyeimlR"
   },
   "source": [
    "<a id='restinfo'></a>\n",
    "# 2. Restaurant Info\n",
    "\n",
    "When filtering on all restaurants in Copenahgen Tripadvisor has a limit of showing 20 restaurants per page. With the help of **selenium** the \"next page\" button is pushed until it has reached the end. We carefully just take all of the resturant's URLS that does not show \"sponsored\" since that would give us duplicates and waste time. The next page button is on the form: \n",
    "\n",
    "<img src=\"webpage_figures/Next_button.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "\n",
    "We get all of the urls first and then iterate through them to first get the reviews and then get the restaurant's info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081b1df0",
   "metadata": {
    "id": "k2kz2XKqimlS"
   },
   "outputs": [],
   "source": [
    "# Get urls for all \"next\" pages in a selected area\n",
    "def scrapeRestaurantsUrlsAll(url, limit=100):\n",
    "    store_name = []\n",
    "    urls = []\n",
    "    limit_set = 1\n",
    "    nextPage = True\n",
    "    while nextPage and limit_set <= limit:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        results = soup.find('div', class_='_1kXteagE')\n",
    "        stores = results.find_all('div', class_='wQjYiB7z') \n",
    "        for store in stores:\n",
    "            if store.find('a', class_ = '_15_ydu6b').text[0].isdigit(): # skip the ones that er sponsored since they will also come later.\n",
    "                \n",
    "                print(store.find('a', class_ = '_15_ydu6b').text)\n",
    "                unModifiedUrl = str(store.find('a', href=True)['href'])\n",
    "                urls.append('https://www.tripadvisor.com'+unModifiedUrl)\n",
    "        limit_set += 1\n",
    "        #Go to next page if exists\n",
    "        try:\n",
    "            print('tried next in finding all')\n",
    "            unModifiedUrl = str(soup.find('a', class_ = 'nav next rndBtn ui_button primary taLnk',href=True)['href'])\n",
    "            # print(unModifiedUrl, 'later unmod')\n",
    "            url = 'https://www.tripadvisor.com' + unModifiedUrl\n",
    "            # print('new url is ', url)\n",
    "        except:\n",
    "            print('no next in finding all')\n",
    "            nextPage = False\n",
    "\n",
    "    with open(pathAllRestaurants, 'wb') as f:\n",
    "        pickle.dump(urls, f)\n",
    "\n",
    "    print(f'Total restaurant count: {len(urls)}')\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1364c725",
   "metadata": {},
   "outputs": [],
   "source": [
    "startingUrl = \"https://www.tripadvisor.com/Restaurants-g189541-Copenhagen_Zealand.html\" # All Copenahagen restaurants\n",
    "urls = scrapeRestaurantsUrlsAll(startingUrl, limit=2300)\n",
    "\n",
    "with open(\"AllRestaurants.txt\", \"rb\") as f:\n",
    "    urls = pickle.dump(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45abe722",
   "metadata": {
    "id": "T03-rCvuimlS"
   },
   "source": [
    "## Scrape the restaurants information \n",
    "\n",
    "The restaurants information is scraped. Here the most applicable data was retrieved and stored into a seperate csv file. Here the **beautifulsoup** package was sufficient to retreive the data needed. Again, here we see many \"try: except:\" clauses in the code since there is some missing information for many of the resturants that would cause the code to fail. In the figure below marked in red boxes is the information we were interested in getting.\n",
    "\n",
    "<img src=\"webpage_figures/restaurant_info.png\" width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed6c50",
   "metadata": {
    "id": "cEOKSyV8imlT"
   },
   "outputs": [],
   "source": [
    "def scrapeRestaurantInfo(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    storeName = soup.find('h1', class_='_3a1XQ88S').text\n",
    "    try:\n",
    "        avgRating = soup.find('span', class_='r2Cf69qf').text.strip()\n",
    "        nrReviews = soup.find('a', class_='_10Iv7dOs').text.strip().split()[0]\n",
    "    except:\n",
    "        avgRating = None\n",
    "        nrReviews = 0\n",
    "    storeAddress = soup.find('div', class_= '_2vbD36Hr _36TL14Jn').find('span', class_='_2saB_OSe').text.strip()\n",
    "#     urlAddress = str(soup.find('div', class_ = '_2vbD36Hr _36TL14Jn').find('span').find('a', href=True)['href'])\n",
    "    \n",
    "    try:\n",
    "        cousineType = [word.text for  word in soup.find('span', class_='_13OzAOXO _34GKdBMV').find_all('a')]\n",
    "        cousine = True\n",
    "    except:\n",
    "        cousineType = []\n",
    "        cousine = False\n",
    "    nrPos = soup.find('a', class_='_15QfMZ2L').find('b').find('span').text.strip()\n",
    "    \n",
    "    # Other rankings \n",
    "    all_ranks = []\n",
    "    try:\n",
    "        all_ranks = [word.text for word in soup.find('div', class_ = '_3acGlZjD').find_all('div', class_ = '_3-W4EexF')]\n",
    "    except:\n",
    "        all_ranks = []\n",
    "        \n",
    "    # Other ratings\n",
    "    all_ratings = []\n",
    "    try:\n",
    "        rating = soup.find_all('div', class_='jT_QMHn2')\n",
    "        rating_type = [x.find('span', class_ = '_2vS3p6SS').text for x in rating]\n",
    "        true_rating = [x.find('span', class_ = '_377onWB-') for x in rating]\n",
    "        true_rating = [int(str(x.findChildren('span')).split('_')[3][:2])/10 for x in true_rating]\n",
    "        all_ratings = list(zip(rating_type,true_rating))\n",
    "    except:\n",
    "        all_ratings = []\n",
    "        \n",
    "    with open(restaurantInfo, mode='a', encoding=\"utf-8\") as trip:\n",
    "        data_writer = csv.writer(trip, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "        if len(cousineType) > 1:\n",
    "            data_writer.writerow([storeName, storeAddress, avgRating, nrReviews, cousineType[0], cousineType[1:], nrPos, all_ranks, all_ratings])\n",
    "        elif len(cousineType) == 1:\n",
    "            data_writer.writerow([storeName, storeAddress, avgRating, nrReviews, cousineType[0], [], nrPos, all_ranks, all_ratings])\n",
    "        else:\n",
    "            data_writer.writerow([storeName, storeAddress, avgRating, nrReviews, [], [], nrPos, all_ranks, all_ratings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382797b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"AllRestaurants.txt\", \"rb\") as f:   # Unpickling\n",
    "    urls = pickle.load(urls)\n",
    "\n",
    "finished = []\n",
    "bad_url = []\n",
    "for url in urls:\n",
    "    try:\n",
    "        scrapeRestaurantInfo(url)\n",
    "        finished.append(url)\n",
    "    except:\n",
    "        bad_url.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4c83ee",
   "metadata": {
    "id": "WnIFaDLJimlV"
   },
   "source": [
    "<a id='reviews'></a>\n",
    "# 2. Reviews\n",
    "\n",
    "A function for getting the reviews was created, it uses selenium to go through all the \"next\" pages since Tripadvisor only displays 10 reviews per page. Selenium is also used to click the \"more\" button when a review is to long to display in the container it is in. Here we can see how there is an extra text available if the more button is pushed.\n",
    "\n",
    "<img src=\"webpage_figures/Before_more.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "Now after using selenium to click on all the **more** buttons on the page that have reviews as their id we can see the whole review and get it.\n",
    "\n",
    "<img src=\"webpage_figures/After_more.png\" width=\"800\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f206284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews(url):\n",
    "    print(url)\n",
    "\n",
    "    nextPage = True\n",
    "    while nextPage:\n",
    "        #Requests\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        #Click More button\n",
    "        more = driver.find_elements_by_xpath(\"//span[@class='taLnk ulBlueLinks'][contains(text(),'More')]\")\n",
    "        # Push all buttons that unclude the \"More\" option on each review.\n",
    "        for x in range(0,len(more)):\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].click();\", more[x])\n",
    "                time.sleep(3)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        #Store name\n",
    "        storeName = soup.find('h1', class_='_3a1XQ88S').text\n",
    "        #Reviews\n",
    "        results = soup.find('div', class_='listContainer hide-more-mobile')\n",
    "        try:\n",
    "            reviews = results.find_all('div', class_='prw_rup prw_reviews_review_resp')\n",
    "        except Exception:\n",
    "            continue\n",
    "        #Export to csv\n",
    "        try:\n",
    "            with open(pathToReviews, mode='a', encoding=\"utf-8\") as trip_data:\n",
    "                data_writer = csv.writer(trip_data, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "                for review in reviews:\n",
    "                    ratingDate = review.find('span', class_='ratingDate').get('title')\n",
    "                    reviewHeader = review.find('span', class_='noQuotes').text\n",
    "                    text_review = review.find('p', class_='partial_entry')\n",
    "                    if len(text_review.contents) > 2:\n",
    "                        reviewText = str(text_review.contents[0][:-3]) + ' ' + str(text_review.contents[1].text)\n",
    "                    else:\n",
    "                        reviewText = text_review.text\n",
    "                    reviewerUsername = review.find('div', class_='info_text pointer_cursor')\n",
    "                    reviewerUsername = reviewerUsername.select('div > div')[0].get_text(strip=True)\n",
    "                    rating = review.find('div', class_='ui_column is-9').findChildren('span')\n",
    "                    rating = str(rating[0]).split('_')[3].split('0')[0]\n",
    "                    data_writer.writerow([storeName, reviewerUsername, ratingDate, reviewHeader, reviewText, rating])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        #Go to next page if exists\n",
    "        try:\n",
    "            unModifiedUrl = str(soup.find('div', class_ = 'prw_rup prw_common_responsive_pagination').find('a', class_='nav next ui_button primary', href=True).get('href'))\n",
    "            # unModifiedUrl = str(soup.find('a', class_ = 'nav next ui_button primary',href=True)['href'])\n",
    "            url = 'https://www.tripadvisor.com' + unModifiedUrl\n",
    "        except:\n",
    "            nextPage = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec6462",
   "metadata": {},
   "source": [
    "Loop through all the restaurants urls and get all reviews for that restaurant, here urls_left is a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca715b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"urls_left.txt\", \"rb\") as f:   # Unpickling\n",
    "    urls_left = pickle.load(f)\n",
    "\n",
    "driver_path = f'{os.getcwd()}/chromedriver' # Get driver to run selenium\n",
    "driver = webdriver.Chrome(driver_path)\n",
    "\n",
    "finished = []\n",
    "bad_url = []\n",
    "url_slice = urls_left[0:100]\n",
    "i = 0\n",
    "for url in url_slice:\n",
    "    try:\n",
    "        get_reviews(url)\n",
    "        finished.append(url)\n",
    "        print(i)\n",
    "        i+=1\n",
    "    except:\n",
    "        bad_url.append(url)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403687a0",
   "metadata": {
    "id": "sxjri0lFimlT"
   },
   "source": [
    "<a id='revinfo'></a>\n",
    "# 4. Reviewer Info\n",
    "\n",
    "\n",
    "After we had stored all the reviews data we gathered all unique reviewers and looked into their profile for additional information. Here **selenium** came to the rescue, as it was necessary to click buttons on the reviewers own page. The information stored here is mainly in the hope to get the connection between reviewers and restaurants. Tripadvisor has a community of reviewers and they can follow each other as on social platforms. The information in that regard is gathered along with the total reviews and \"upvotes\" the reviewer gives. The hope here is to shed light on the influence of specific reviewers and the value it could add to restaurants. Here detecting bad or fraudulent reviews is hopefully possible with the data at hand. The most frequent available data is the location and the join date of the reviewer. This information is quite important since a network can be created based on those attributes.\n",
    "\n",
    "\n",
    "The lyout of the reviewer is on the form: \n",
    "\n",
    "<img src=\"webpage_figures/Reviewer.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "We have multiple field that we want to get but some of them are hidden, when we click on contributions we get the following window: \n",
    "\n",
    "<img src=\"webpage_figures/Reviewer_cont.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "and subequently when clickin on follower or following we get what people are at play.\n",
    "\n",
    "<img src=\"webpage_figures/Reviewer_following.png\" width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac366da",
   "metadata": {
    "id": "bV6sMhqCimlU"
   },
   "outputs": [],
   "source": [
    "# Get all the reviwer info, location, join date, review count, upvotes, followers and following.\n",
    "def reviewerInfo(url):\n",
    "    username = url\n",
    "    full_url = f\"https://www.tripadvisor.com/Profile/{url}\"\n",
    "    driver.get(full_url)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Get Intro info, location and join date\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    try:\n",
    "        location = soup.find('span', class_ = \"_2VknwlEe _3J15flPT default\").text\n",
    "    except:\n",
    "        location = None\n",
    "\n",
    "    try:\n",
    "        joined = soup.find('span', class_ = \"_1CdMKu4t\").text\n",
    "    except:\n",
    "        joined = None\n",
    "\n",
    "\n",
    "    all_links = soup.find_all('div', class_ = '_1aVEDY08')\n",
    "    # link = driver.find_elements_by_xpath(\"//div[@class='nkw-3XeH']/div[1]/span[2]/a\")\n",
    "\n",
    "    # # Get the contributions info\n",
    "    nrContributions = int(str(all_links[0].text).split()[1])\n",
    "    if nrContributions > 0:\n",
    "        link = driver.find_elements_by_xpath(\"//div[@class='nkw-3XeH']/div[1]/span[2]/a\")\n",
    "        driver.execute_script(\"arguments[0].click();\", link[0])\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        nrReviews = int(str(soup.find('span', class_ = 'ui_icon pencil-paper _1LSVmZLi').parent.text).split()[0])\n",
    "        try:\n",
    "            nrUpvotes = int(str(soup.find('span', class_ ='ui_icon thumbs-up _1LSVmZLi _3zmXi7gU').parent.text).split()[0])\n",
    "        except:\n",
    "            nrUpvotes = 0\n",
    "        close = driver.find_elements_by_xpath(\"//div[@class='_2EFRp_bb _9Wi4Mpeb']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", close[0])\n",
    "    else:\n",
    "        nrReviews = 0\n",
    "        nrUpvotes = 0\n",
    "\n",
    "    # Get Followers\n",
    "    nrFollowers = int(str(all_links[1].text).split()[1])\n",
    "    if nrFollowers > 0:\n",
    "        link = driver.find_elements_by_xpath(\"//div[@class='nkw-3XeH']/div[2]/span[2]/a\")\n",
    "        driver.execute_script(\"arguments[0].click();\", link[0])\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        followers = [word.text for word in soup.find('div', class_='_1caczhWN').find_all('span', class_='gf69u3Nd')]\n",
    "        close = driver.find_elements_by_xpath(\"//div[@class='_2EFRp_bb _9Wi4Mpeb']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", close[0])\n",
    "    else:\n",
    "        followers = []\n",
    "\n",
    "    # Get all following\n",
    "    nrFollowing = int(str(all_links[2].text).split()[1])\n",
    "    if nrFollowing > 0:\n",
    "        link = driver.find_elements_by_xpath(\"//div[@class='nkw-3XeH']/div[3]/span[2]/a\")\n",
    "        driver.execute_script(\"arguments[0].click();\", link[0])\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        following = [word.text for word in soup.find('div', class_='_1caczhWN').find_all('span', class_='gf69u3Nd')]\n",
    "        close = driver.find_elements_by_xpath(\"//div[@class='_2EFRp_bb _9Wi4Mpeb']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", close[0])\n",
    "    else:\n",
    "        following = []\n",
    "\n",
    "    with open(pathtoReviewers, mode='a', encoding=\"utf-8\") as reviewer_data:\n",
    "        data_writer = csv.writer(reviewer_data, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "        data_writer.writerow([username, location, joined, nrContributions,nrReviews, nrUpvotes, nrFollowers, followers, nrFollowing,following])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0f34d5",
   "metadata": {},
   "source": [
    "Get all the reviewers from our reviews data-set, here we need to merge all of our csv files together since we did many paralell runs to get the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b1f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = csv.reader(open(\"reviews1.csv\"))\n",
    "\n",
    "f = open(\"reviews_all.csv\", \"w\")\n",
    "writer = csv.writer(f)\n",
    "\n",
    "for row in reader:\n",
    "    writer.writerow(row)\n",
    "\n",
    "for x in range(2,12):\n",
    "    file = f\"reviews{x}.csv\"\n",
    "    reader = csv.reader(open(file))\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        writer.writerow(row)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "successful-theta",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>storeName</th>\n",
       "      <th>reviewerUsername</th>\n",
       "      <th>ratingDate</th>\n",
       "      <th>reviewHeader</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maple Casual Dining</td>\n",
       "      <td>918emmaf</td>\n",
       "      <td>December 5, 2020</td>\n",
       "      <td>Exquisite</td>\n",
       "      <td>We visited Maple in Friday night and had a won...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maple Casual Dining</td>\n",
       "      <td>hildurj2016</td>\n",
       "      <td>November 19, 2020</td>\n",
       "      <td>Perfect wedding dinner</td>\n",
       "      <td>Excellent food, drinks and service!! Me and my...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Maple Casual Dining</td>\n",
       "      <td>Judy B</td>\n",
       "      <td>October 27, 2020</td>\n",
       "      <td>Beautifully Presented Food</td>\n",
       "      <td>I visited this restaurant on my first ever vis...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maple Casual Dining</td>\n",
       "      <td>EldBjoern</td>\n",
       "      <td>October 18, 2020</td>\n",
       "      <td>Very good food and very pleasant people</td>\n",
       "      <td>We ate dinner in their restaurant. The waiter ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maple Casual Dining</td>\n",
       "      <td>MacondoExpresss</td>\n",
       "      <td>October 13, 2020</td>\n",
       "      <td>A lovely birthday dinner</td>\n",
       "      <td>Visited as a couple to celebrate my birthday. ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             storeName reviewerUsername         ratingDate  \\\n",
       "0  Maple Casual Dining         918emmaf   December 5, 2020   \n",
       "1  Maple Casual Dining      hildurj2016  November 19, 2020   \n",
       "2  Maple Casual Dining           Judy B   October 27, 2020   \n",
       "3  Maple Casual Dining        EldBjoern   October 18, 2020   \n",
       "4  Maple Casual Dining  MacondoExpresss   October 13, 2020   \n",
       "\n",
       "                              reviewHeader  \\\n",
       "0                                Exquisite   \n",
       "1                   Perfect wedding dinner   \n",
       "2               Beautifully Presented Food   \n",
       "3  Very good food and very pleasant people   \n",
       "4                 A lovely birthday dinner   \n",
       "\n",
       "                                          reviewText  rating  \n",
       "0  We visited Maple in Friday night and had a won...       5  \n",
       "1  Excellent food, drinks and service!! Me and my...       5  \n",
       "2  I visited this restaurant on my first ever vis...       5  \n",
       "3  We ate dinner in their restaurant. The waiter ...       5  \n",
       "4  Visited as a couple to celebrate my birthday. ...       5  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"all_reviews.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c795cd8f",
   "metadata": {},
   "source": [
    "We only want to look at unique usernames an we see that we have **54.541** unique reviewers in our data-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cleared-shepherd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54541\n"
     ]
    }
   ],
   "source": [
    "all_reviewers = list(df.reviewerUsername.unique())\n",
    "print(len(all_reviewers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "practical-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the reviewers we want data about\n",
    "with open('all_reviewers.txt', 'wb') as f:\n",
    "    pickle.dump(all_reviewers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb122d00",
   "metadata": {},
   "source": [
    "No we loop through each reviewer id and use our function to retrieve the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc36eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chrome driver with selenium\n",
    "driver_path = f'{os.getcwd()}/chromedriver'\n",
    "driver = webdriver.Chrome(driver_path)\n",
    "\n",
    "with open(\"all_reviewers.txt\", \"r\") as f:\n",
    "    reviewers = f.readlines()\n",
    "\n",
    "bad_url = []\n",
    "reviewer_slice = reviewers[0:10000] # Take a slice of 10000 reviewers\n",
    "for reviewer in reviewer_slice:\n",
    "    try:\n",
    "        reviewerInfo(reviewer)\n",
    "    except:\n",
    "        bad_url.append(reviewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-heart",
   "metadata": {},
   "source": [
    "#### Since this was done in many runs we now merge all csv files of reviewer info together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "proof-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = csv.reader(open(\"reviewer_info/reviewerInfo_1.csv\"))\n",
    "\n",
    "f = open(\"all_reviewer_info.csv\", \"w\")\n",
    "writer = csv.writer(f)\n",
    "\n",
    "for row in reader:\n",
    "    writer.writerow(row)\n",
    "\n",
    "for x in range(2,6):\n",
    "    file = f\"reviewer_info/reviewerInfo_{x}.csv\"\n",
    "    reader = csv.reader(open(file))\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        writer.writerow(row)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a55b6",
   "metadata": {},
   "source": [
    "<a id='improv'></a>\n",
    "# 5. Improvements\n",
    "\n",
    "The total time that took to run all of the scraping needed was about 45 hours. You try your best to think of all exceptions or data that need to be stored but sometimes you overlook somethin that later turns out to be very valuable or even essential for your work. In our case we saw a flaw in our \"GetReviews\" function where the restaurant individual ID is not stores anywhere, though is was easily available. This caused reviews for restaurant chains to not be identifiable to a specific restaurant since they have the same name. We moved on even though this mistake was found since all the data was allready scraped and we take this as a lesson to more carefully uniquely identafy something that could conflict with other data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
