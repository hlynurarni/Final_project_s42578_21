{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "julian-security",
   "metadata": {},
   "source": [
    "### Scraping tool to get reviews from restaurants. Firstly the function scrapeRestaurantsUrlsAll is used to gether all the restaurant urls based on location. Then we iterate through those urls and collect all the reviews.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-qatar",
   "metadata": {},
   "source": [
    "### Keyrðu cellurnar þangað til þú sérð"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "three-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import csv \n",
    "from selenium import webdriver\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "normal-actress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add urls for all \"next\" pages\n",
    "def scrapeRestaurantsUrls(tripURLs):\n",
    "    urls =[]\n",
    "    for url in tripURLs:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        results = soup.find('div', class_='_1kXteagE')\n",
    "        stores = results.find_all('div', class_='wQjYiB7z')\n",
    "        for store in stores:\n",
    "            unModifiedUrl = str(store.find('a', href=True)['href'])\n",
    "            urls.append('https://www.tripadvisor.com'+unModifiedUrl)\n",
    "    return urls\n",
    "\n",
    "# Add urls for all \"next\" pages\n",
    "def scrapeRestaurantsUrlsAll(url, limit=100):\n",
    "    store_name = []\n",
    "    urls = []\n",
    "    limit_set = 1\n",
    "    nextPage = True\n",
    "    while nextPage and limit_set <= limit:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        results = soup.find('div', class_='_1kXteagE')\n",
    "        stores = results.find_all('div', class_='wQjYiB7z') \n",
    "        for store in stores:\n",
    "            if store.find('a', class_ = '_15_ydu6b').text[0].isdigit(): # skip the ones that er sponsored since they will also come later.\n",
    "                \n",
    "                print(store.find('a', class_ = '_15_ydu6b').text)\n",
    "                unModifiedUrl = str(store.find('a', href=True)['href'])\n",
    "                urls.append('https://www.tripadvisor.com'+unModifiedUrl)\n",
    "        limit_set += 1\n",
    "        #Go to next page if exists\n",
    "        try:\n",
    "            print('tried next in finding all')\n",
    "            unModifiedUrl = str(soup.find('a', class_ = 'nav next rndBtn ui_button primary taLnk',href=True)['href'])\n",
    "            # print(unModifiedUrl, 'later unmod')\n",
    "            url = 'https://www.tripadvisor.com' + unModifiedUrl\n",
    "            # print('new url is ', url)\n",
    "        except:\n",
    "            print('no next in finding all')\n",
    "            nextPage = False\n",
    "\n",
    "    with open(pathAllRestaurants, 'wb') as f:\n",
    "        pickle.dump(urls, f)\n",
    "\n",
    "    print(f'Total restaurant count: {len(urls)}')\n",
    "    return urls\n",
    "\n",
    "def scrapeRestaurantInfo(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    storeName = soup.find('h1', class_='_3a1XQ88S').text\n",
    "    avgRating = soup.find('span', class_='r2Cf69qf').text.strip()\n",
    "    storeAddress = soup.find('div', class_= '_2vbD36Hr _36TL14Jn').find('span', class_='_2saB_OSe').text.strip()\n",
    "#     urlAddress = str(soup.find('div', class_ = '_2vbD36Hr _36TL14Jn').find('span').find('a', href=True)['href'])\n",
    "    nrReviews = soup.find('a', class_='_10Iv7dOs').text.strip().split()[0]\n",
    "    cousineType = [word.text for  word in soup.find('span', class_='_13OzAOXO _34GKdBMV').find_all('a')]\n",
    "    nrPos = soup.find('a', class_='_15QfMZ2L').find('b').find('span').text.strip()\n",
    "    with open(pathToStoreInfo, mode='a', encoding=\"utf-8\") as trip:\n",
    "        data_writer = csv.writer(trip, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "        data_writer.writerow([storeName, storeAddress, avgRating, nrReviews, cousineType[0], cousineType[1:], nrPos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "solved-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Chrome('chromedriver.exe')\n",
    "# This should be set to the path of the folder holding the python script.\n",
    "    \n",
    "def get_reviews(url):\n",
    "    print(url)\n",
    "    #if you want to scrape restaurants info\n",
    "\n",
    "#     scrapeRestaurantInfo(url)\n",
    "\n",
    "    nextPage = True\n",
    "    while nextPage:\n",
    "        #Requests\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        #Click More button\n",
    "#         more = driver.find_elements_by_xpath(\"//span[contains(text(),'More')]\")\n",
    "        more = driver.find_elements_by_xpath(\"//span[@class='taLnk ulBlueLinks'][contains(text(),'More')]\")\n",
    "        # Push all buttons that unclude the \"More\" option on each review.\n",
    "        for x in range(0,len(more)):\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].click();\", more[x])\n",
    "                time.sleep(3)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        #Store name\n",
    "        storeName = soup.find('h1', class_='_3a1XQ88S').text\n",
    "        #Reviews\n",
    "        results = soup.find('div', class_='listContainer hide-more-mobile')\n",
    "        try:\n",
    "            reviews = results.find_all('div', class_='prw_rup prw_reviews_review_resp')\n",
    "        except Exception:\n",
    "            continue\n",
    "        #Export to csv\n",
    "        try:\n",
    "            with open(pathToReviews, mode='a', encoding=\"utf-8\") as trip_data:\n",
    "                data_writer = csv.writer(trip_data, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "                for review in reviews:\n",
    "                    ratingDate = review.find('span', class_='ratingDate').get('title')\n",
    "                    reviewHeader = review.find('span', class_='noQuotes').text\n",
    "                    text_review = review.find('p', class_='partial_entry')\n",
    "                    if len(text_review.contents) > 2:\n",
    "                        reviewText = str(text_review.contents[0][:-3]) + ' ' + str(text_review.contents[1].text)\n",
    "                    else:\n",
    "                        reviewText = text_review.text\n",
    "                    reviewerUsername = review.find('div', class_='info_text pointer_cursor')\n",
    "                    reviewerUsername = reviewerUsername.select('div > div')[0].get_text(strip=True)\n",
    "                    rating = review.find('div', class_='ui_column is-9').findChildren('span')\n",
    "                    rating = str(rating[0]).split('_')[3].split('0')[0]\n",
    "                    data_writer.writerow([storeName, reviewerUsername, ratingDate, reviewHeader, reviewText, rating])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        #Go to next page if exists\n",
    "        try:\n",
    "            unModifiedUrl = str(soup.find('div', class_ = 'prw_rup prw_common_responsive_pagination').find('a', class_='nav next ui_button primary', href=True).get('href'))\n",
    "            # unModifiedUrl = str(soup.find('a', class_ = 'nav next ui_button primary',href=True)['href'])\n",
    "            url = 'https://www.tripadvisor.com' + unModifiedUrl\n",
    "        except:\n",
    "            nextPage = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-junior",
   "metadata": {},
   "source": [
    "## INFO\n",
    "\n",
    "Þú getur valið hversu mikið þú tekur í keyrslunni en ég held að 300 ætti alveg að vera fínt. \n",
    "\n",
    "Það er hægt að copy-a þessu notebook og keyra 2 samtímis þá breytiru bara 0:150 og 150:300 fyrir hvoru keyrsluna.\n",
    "\n",
    "Ég sé svo um að eyða út fyrstu 300 línunum og skoða svo þau url sem voru með vesen \"Save-a url með vesen\". Þú mátt runna þá cellu einnig svo vandræðaslóðirnar séu vistaðar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "reliable-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new reviews.csv path for run\n",
    "pathToReviews = \"reviews5_kristin.csv\"\n",
    "\n",
    "with open(pathToReviews, mode='a', encoding=\"utf-8\") as trip_data:\n",
    "    data_writer = csv.writer(trip_data, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "    data_writer.writerow(['storeName', 'reviewerUsername', 'ratingDate', 'reviewHeader','reviewText', 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-grenada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Þetta er aðalskjalið sem heldur utan um þá linka sem eru eftir.\n",
    "with open(\"urls_left.txt\", \"rb\") as f:   # Unpickling\n",
    "    urls_left = pickle.load(f)\n",
    "\n",
    "driver_path = f'{os.getcwd()}/chromedriver' # Get driver to run selenium\n",
    "driver = webdriver.Chrome(driver_path)\n",
    "\n",
    "finished = []\n",
    "bad_url = []\n",
    "rest_300 = urls_left[0:300]\n",
    "for url in rest_300:\n",
    "    try:\n",
    "        get_reviews(url)\n",
    "        finished.append(url)\n",
    "    except:\n",
    "        bad_url.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-seattle",
   "metadata": {},
   "source": [
    "## Save-a url með vesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For curiosity how many urls didn't get scraped\n",
    "print(len(bad_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the urls that are left to scrape\n",
    "with open('bad_url.txt', 'wb') as f:\n",
    "    pickle.dump(bad_url, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-dating",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-criterion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
