{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "outer-vulnerability",
   "metadata": {},
   "source": [
    "### In this notebook we hava created a scraping tool for tripadvisor. This notebooks is an addition to the review scarping tool to gain additional information about restaurants and reviewers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "induced-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import csv \n",
    "from selenium import webdriver\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "received-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the urls needed to scrape\n",
    "with open('AllRestaurants.txt', \"rb\") as f:   # Unpickling\n",
    "    urls = pickle.load(f)\n",
    "\n",
    "pathtoReviewers = \"reviewerInfo.csv\"\n",
    "with open(pathtoReviewers, mode='a', encoding=\"utf-8\") as reviewer_data:\n",
    "    data_writer = csv.writer(reviewer_data, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "    data_writer.writerow(['username', 'location', 'joined', 'nrContributions','nrReviews', 'nrUpvotes', 'nrFollowers','followers', 'nrFollowing','following'])\n",
    "\n",
    "pathToStoreInfo = \"restaurantInfo.csv\"\n",
    "with open(pathToStoreInfo, mode='a', encoding=\"utf-8\") as trip:\n",
    "    data_writer = csv.writer(trip, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "    data_writer.writerow(['storeName', 'storeAddress', 'avgRating', 'nrReviews', 'priceCategory','CousineType', 'all_ranks', 'all_ratings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "faced-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeRestaurantInfo(url):\n",
    "    page = requests.get(url, timeout=10)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    storeName = soup.find('h1', class_='_3a1XQ88S').text\n",
    "    try:\n",
    "        avgRating = soup.find('span', class_='r2Cf69qf').text.strip()\n",
    "        nrReviews = soup.find('a', class_='_10Iv7dOs').text.strip().split()[0]\n",
    "    except:\n",
    "        avgRating = None\n",
    "        nrReviews = 0\n",
    "    storeAddress = soup.find('div', class_= '_2vbD36Hr _36TL14Jn').find('span', class_='_2saB_OSe').text.strip()\n",
    "#     urlAddress = str(soup.find('div', class_ = '_2vbD36Hr _36TL14Jn').find('span').find('a', href=True)['href'])\n",
    "    \n",
    "    try:\n",
    "        cousineType = [word.text for  word in soup.find('span', class_='_13OzAOXO _34GKdBMV').find_all('a')]\n",
    "        cousine = True\n",
    "        if cousineType[0][0] != \"$\":\n",
    "            cousineType.insert(0, '')\n",
    "        \n",
    "    except:\n",
    "        cousineType = []\n",
    "        cousine = False\n",
    "#     nrPos = soup.find('a', class_='_15QfMZ2L').find('b').find('span').text.strip()\n",
    "    \n",
    "    # Other rankings \n",
    "    all_ranks = []\n",
    "    try:\n",
    "        all_ranks = [word.text for word in soup.find('div', class_ = '_3acGlZjD').find_all('div', class_ = '_3-W4EexF')]\n",
    "    except:\n",
    "        all_ranks = []\n",
    "        \n",
    "    # Other ratings\n",
    "    all_ratings = []\n",
    "    try:\n",
    "        rating = soup.find_all('div', class_='jT_QMHn2')\n",
    "        rating_type = [x.find('span', class_ = '_2vS3p6SS').text for x in rating]\n",
    "        true_rating = [x.find('span', class_ = '_377onWB-') for x in rating]\n",
    "        true_rating = [int(str(x.findChildren('span')).split('_')[3][:2])/10 for x in true_rating]\n",
    "        all_ratings = list(zip(rating_type,true_rating))\n",
    "    except:\n",
    "        all_ratings = []\n",
    "    with open(pathToStoreInfo, mode='a', encoding=\"utf-8\") as trip:\n",
    "        data_writer = csv.writer(trip, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "        if len(cousineType) > 1:\n",
    "            data_writer.writerow([storeName, storeAddress, avgRating, nrReviews, cousineType[0], cousineType[1:], all_ranks, all_ratings])\n",
    "        elif len(cousineType) == 1:\n",
    "            data_writer.writerow([storeName, storeAddress, avgRating, nrReviews, cousineType[0], [], all_ranks, all_ratings])\n",
    "        else:\n",
    "            data_writer.writerow([storeName, storeAddress, avgRating, nrReviews, '', [], all_ranks, all_ratings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "noted-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"AllRestaurants.txt\", \"rb\") as f:   # Unpickling\n",
    "    urls = pickle.load(f)\n",
    "\n",
    "bad_url = []\n",
    "for url in urls:\n",
    "    try:\n",
    "        scrapeRestaurantInfo(url)\n",
    "    except:\n",
    "        bad_url.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "leading-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEt all the reviwer info, location, join date, review count, upvotes, followers and following.\n",
    "def reviewerInfo(url):\n",
    "    username = url\n",
    "    full_url = f\"https://www.tripadvisor.com/Profile/{url}\"\n",
    "    driver.get(full_url)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Get Intro info, location and join date\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    try:\n",
    "        location = soup.find('span', class_ = \"_2VknwlEe _3J15flPT default\").text\n",
    "    except:\n",
    "        location = None\n",
    "\n",
    "    try:\n",
    "        joined = soup.find('span', class_ = \"_1CdMKu4t\").text\n",
    "    except:\n",
    "        joined = None\n",
    "\n",
    "\n",
    "    all_links = soup.find_all('div', class_ = '_1aVEDY08')\n",
    "    # link = driver.find_elements_by_xpath(\"//div[@class='nkw-3XeH']/div[1]/span[2]/a\")\n",
    "\n",
    "    # # Get the contributions info\n",
    "    nrContributions = int(str(all_links[0].text).split()[1])\n",
    "    if nrContributions > 0:\n",
    "        link = driver.find_elements_by_xpath(\"//div[@class='nkw-3XeH']/div[1]/span[2]/a\")\n",
    "        driver.execute_script(\"arguments[0].click();\", link[0])\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        nrReviews = int(str(soup.find('span', class_ = 'ui_icon pencil-paper _1LSVmZLi').parent.text).split()[0])\n",
    "        try:\n",
    "            nrUpvotes = int(str(soup.find('span', class_ ='ui_icon thumbs-up _1LSVmZLi _3zmXi7gU').parent.text).split()[0])\n",
    "        except:\n",
    "            nrUpvotes = 0\n",
    "        close = driver.find_elements_by_xpath(\"//div[@class='_2EFRp_bb _9Wi4Mpeb']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", close[0])\n",
    "    else:\n",
    "        nrReviews = 0\n",
    "        nrUpvotes = 0\n",
    "\n",
    "    # Get Followers\n",
    "    nrFollowers = int(str(all_links[1].text).split()[1])\n",
    "    if nrFollowers > 0:\n",
    "        link = driver.find_elements_by_xpath(\"//div[@class='nkw-3XeH']/div[2]/span[2]/a\")\n",
    "        driver.execute_script(\"arguments[0].click();\", link[0])\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        followers = [word.text for word in soup.find('div', class_='_1caczhWN').find_all('span', class_='gf69u3Nd')]\n",
    "        close = driver.find_elements_by_xpath(\"//div[@class='_2EFRp_bb _9Wi4Mpeb']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", close[0])\n",
    "    else:\n",
    "        followers = []\n",
    "\n",
    "    # Get all following\n",
    "    nrFollowing = int(str(all_links[2].text).split()[1])\n",
    "    if nrFollowing > 0:\n",
    "        link = driver.find_elements_by_xpath(\"//div[@class='nkw-3XeH']/div[3]/span[2]/a\")\n",
    "        driver.execute_script(\"arguments[0].click();\", link[0])\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        following = [word.text for word in soup.find('div', class_='_1caczhWN').find_all('span', class_='gf69u3Nd')]\n",
    "        close = driver.find_elements_by_xpath(\"//div[@class='_2EFRp_bb _9Wi4Mpeb']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", close[0])\n",
    "    else:\n",
    "        following = []\n",
    "\n",
    "    with open(pathtoReviewers, mode='a', encoding=\"utf-8\") as reviewer_data:\n",
    "        data_writer = csv.writer(reviewer_data, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "        data_writer.writerow([username, location, joined, nrContributions,nrReviews, nrUpvotes, nrFollowers, followers, nrFollowing,following])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_path = f'{os.getcwd()}/chromedriver'\n",
    "driver = webdriver.Chrome(driver_path)\n",
    "\n",
    "with open(\"reviewers.txt\", \"r\") as f:\n",
    "    reviewers = f.readlines()\n",
    "\n",
    "for reviewer in reviewers:\n",
    "    reviewerInfo(reviewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "under-rings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.tripadvisor.com/Restaurant_Review-g189541-d10127436-Reviews-Skagen_Fiskerestaurant_Illum_Rooftop-Copenhagen_Zealand.html'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "complete-being",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://maps.google.com/maps?saddr=&daddr=Refshalevej+96%2C+Copenhagen+1432+Denmark@55.68283,12.610479'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of how to get the href from googlapis, load the website with selenium and then pull the href attribute.\n",
    "\n",
    "driver = webdriver.Chrome('/Users/hlynurarnisigurjonsson/Downloads/TripAdvisor-Python-Scraper-Restaurants-2021-main/chromedriver')\n",
    "url = \"https://www.tripadvisor.com/Restaurant_Review-g189541-d694971-Reviews-Noma-Copenhagen_Zealand.html\"\n",
    "driver.get(url)\n",
    "time.sleep(1)\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# more = driver.find_elements_by_xpath(\"//div[@class='taLnk ulBlueLinks']/a\")\n",
    "# more['href']\n",
    "# page = requests.get(url)\n",
    "\n",
    "# soup = BeautifulSoup(page.text, 'html.parser')\n",
    "soup.find('div', class_ = '_2vbD36Hr _36TL14Jn').find('a').get('href')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
