{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                                   Helga Sigr√≠√∞ur Thordersen Magn√∫sd√≥ttir s202027 \n",
    "                                                                                 Hlynur √Årni Sigurj√≥nsson s192302\n",
    "                                                                             Katr√≠n Erla Bergsveinsd√≥ttir s202026\n",
    "                                                                                Krist√≠n Bj√∂rk Lilliendahl s192296\n",
    " \n",
    " ![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://susfans.eu/sites/default/files/clients/DTU.png\"  align=\"right\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Business Analytics - Project\n",
    "\n",
    "## Tripadvisor - Copenhagen restaurants reviews\n",
    "\n",
    "<br>\n",
    "The idea is to scrape tripadvisor reviews of restaurants in copenhagen. \n",
    "\n",
    "#### The information gained would be\n",
    "\n",
    "* Review text, rating and time\n",
    "* The resturants info\n",
    "* Basic reviewer info.\n",
    "\n",
    "#### I our thoughts of what could be done with the data.\n",
    "\n",
    "* Create a network of reviewers and restaurants\n",
    "* Sentiment analysis on the reviews\n",
    "* Word embedding on the reviews\n",
    "* Words accosiated with bad and good reviews\n",
    "* Recommendation system based on ratings\n",
    "* Time laps of reviews based on location\n",
    "* Spacial prediction and trends\n",
    "* Fraud detection (detecting un-authentic reviews), would be subjective I assume\n",
    "* Are there any neighborhoods better rated than others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* [1 Scraper info](#scraper)\n",
    "* [2 The datasets and data preparation](#datasets)\n",
    "    * [2.1 restaurantInfo.csv](#restaurantInfo)\n",
    "    * [2.2 reviews.csv](#reviews)\n",
    "    * [2.3 reviewerInfo.csv](#reviewer)\n",
    "    * [2.4 Shapefiles](#shapefiles)\n",
    "* [3 Descriptive stats](#descStats)\n",
    "    * [3.1 Restaurants](#restaurants)\n",
    "    * [3.2 Reviews](#reviews)\n",
    "* [4 Business questions](#businessquestions)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imported data üêº \n",
    "The first step as always is to install and import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas_profiling\n",
    "# !pip install folium\n",
    "# !pip install -U selenium\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from pandas_profiling import ProfileReport\n",
    "import requests\n",
    "import urllib.parse\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "#  Scraper packages\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import csv \n",
    "from selenium import webdriver\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scraper'></a>\n",
    "# 1. Scraper info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scraper tool is created to gather the reviews and the restaurants info into csv files. The skeleton of the tool was taken from a github page [LaskasP](https://github.com/LaskasP/TripAdvisor-Python-Scraper-Restaurants-2021). It later turned out that the code had many errors and crashed after a few calls. So the scarper tool was fixed and improved, with enriching the information gathered. The addition of getting reviewer information was created. Since the data was much and the tool was expected to encounter some errors on the way the urls links are stored in a csv. In our case we chose to look into resturant in the Copenhagen area, with over 1900 resturants available. Each restaurant has on average 700 reviews so the scraping time is quick to add up.\n",
    "\n",
    "The scraper is based on the beautifulsoup package and selenium. The reason for using selenium is to open and click on  things, to retriew next pages or additional information.\n",
    "\n",
    "#### Selenium actions\n",
    "* Click next button since tripadvisor only displays 20 restaurants or reviews at each page\n",
    "* Click boxes that pop out with addition information about reviewers\n",
    "* Click on the \"more\" button when a review is exceeding a certain length\n",
    "\n",
    "#### Procedure.\n",
    "\n",
    "* Find a tripadvisor page with a selected area and select only restaurants\n",
    "* Run the scrapeRestaurantsUrlsAll function, this function retrievs all the urls in the selected area\n",
    "* Run through all the urls and scrape the reviews with get_reviews function\n",
    "* If successful retrieval of all reviews remove the the resturant urls csv file\n",
    "* Sperately run the scrapeRestaurantInfo function to get the information of the restaurants\n",
    "\n",
    "As can be seen in the three code snippets below there are a lot of \"try: except:\" clauses in the code. This is do to many smaller deviation in the tripadvisor webpage. Data can be missing for some restaurants so the scraper tries to retrieve them, if not successfull it is left empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape all restaurants urls\n",
    "\n",
    "With the help of selenium the next page button is pushed until it has reached the end. Every resturant's url that is not \"sponsored\" in the ordering is saved. The sponsored restaurant appear many times and ofter the same restaurants, if this would be skipped that data would have a lot of duplicates but what is worse it would extend the scraper tools to by a big margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get urls for all \"next\" pages in a selected area\n",
    "def scrapeRestaurantsUrlsAll(url, limit=100):\n",
    "    store_name = []\n",
    "    urls = []\n",
    "    limit_set = 1\n",
    "    nextPage = True\n",
    "    while nextPage and limit_set <= limit:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        results = soup.find('div', class_='_1kXteagE')\n",
    "        stores = results.find_all('div', class_='wQjYiB7z') \n",
    "        for store in stores:\n",
    "            if store.find('a', class_ = '_15_ydu6b').text[0].isdigit(): # skip the ones that er sponsored since they will also come later.\n",
    "                \n",
    "                print(store.find('a', class_ = '_15_ydu6b').text)\n",
    "                unModifiedUrl = str(store.find('a', href=True)['href'])\n",
    "                urls.append('https://www.tripadvisor.com'+unModifiedUrl)\n",
    "        limit_set += 1\n",
    "        #Go to next page if exists\n",
    "        try:\n",
    "            print('tried next in finding all')\n",
    "            unModifiedUrl = str(soup.find('a', class_ = 'nav next rndBtn ui_button primary taLnk',href=True)['href'])\n",
    "            # print(unModifiedUrl, 'later unmod')\n",
    "            url = 'https://www.tripadvisor.com' + unModifiedUrl\n",
    "            # print('new url is ', url)\n",
    "        except:\n",
    "            print('no next in finding all')\n",
    "            nextPage = False\n",
    "\n",
    "    with open(pathAllRestaurants, 'wb') as f:\n",
    "        pickle.dump(urls, f)\n",
    "\n",
    "    print(f'Total restaurant count: {len(urls)}')\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape the restaurants info\n",
    "\n",
    "The restaurants info is scraped. Here the most applicable data was retrieved and stored into a seperate csv file. Here the beautifulsoup package was sufficient to retreived the data needed. Again here we see many try: except: clauses in the code since there is missing information for many of the resturants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeRestaurantInfo(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    storeName = soup.find('h1', class_='_3a1XQ88S').text\n",
    "    try:\n",
    "        avgRating = soup.find('span', class_='r2Cf69qf').text.strip()\n",
    "        nrReviews = soup.find('a', class_='_10Iv7dOs').text.strip().split()[0]\n",
    "    except:\n",
    "        avgRating = None\n",
    "        nrReviews = 0\n",
    "    storeAddress = soup.find('div', class_= '_2vbD36Hr _36TL14Jn').find('span', class_='_2saB_OSe').text.strip()\n",
    "#     urlAddress = str(soup.find('div', class_ = '_2vbD36Hr _36TL14Jn').find('span').find('a', href=True)['href'])\n",
    "    \n",
    "    try:\n",
    "        cousineType = [word.text for  word in soup.find('span', class_='_13OzAOXO _34GKdBMV').find_all('a')]\n",
    "        cousine = True\n",
    "    except:\n",
    "        cousineType = []\n",
    "        cousine = False\n",
    "    nrPos = soup.find('a', class_='_15QfMZ2L').find('b').find('span').text.strip()\n",
    "    \n",
    "    # Other rankings \n",
    "    all_ranks = []\n",
    "    try:\n",
    "        all_ranks = [word.text for word in soup.find('div', class_ = '_3acGlZjD').find_all('div', class_ = '_3-W4EexF')]\n",
    "    except:\n",
    "        all_ranks = []\n",
    "        \n",
    "    # Other ratings\n",
    "    all_ratings = []\n",
    "    try:\n",
    "        rating = soup.find_all('div', class_='jT_QMHn2')\n",
    "        rating_type = [x.find('span', class_ = '_2vS3p6SS').text for x in rating]\n",
    "        true_rating = [x.find('span', class_ = '_377onWB-') for x in rating]\n",
    "        true_rating = [int(str(x.findChildren('span')).split('_')[3][:2])/10 for x in true_rating]\n",
    "        all_ratings = list(zip(rating_type,true_rating))\n",
    "    except:\n",
    "        all_ratings = []\n",
    "        \n",
    "    with open(restaurantInfo, mode='a', encoding=\"utf-8\") as trip:\n",
    "        data_writer = csv.writer(trip, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "        if len(cousineType) > 1:\n",
    "            data_writer.writerow([storeName, storeAddress, avgRating, nrReviews, cousineType[0], cousineType[1:], nrPos, all_ranks, all_ratings])\n",
    "        elif len(cousineType) == 1:\n",
    "            data_writer.writerow([storeName, storeAddress, avgRating, nrReviews, cousineType[0], [], nrPos, all_ranks, all_ratings])\n",
    "        else:\n",
    "            data_writer.writerow([storeName, storeAddress, avgRating, nrReviews, [], [], nrPos, all_ranks, all_ratings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the info of the reviewer\n",
    "\n",
    "Here selenium came to the rescue as the need to click buttons on the reviewers own page was neccessary. The infor store here is mainly in the hope to get the connection between reviewers and restaurants. Tripadvisor has a community of reviewers and they can follow each other as on social platforms. The information in that regard is gathered along with the total reviews and \"upvotes\" the reviewer gives. The hope here is to shed light on the influence of specific reviewers and the value it could add to restaurants. Here detecting bad or fraudulent reviews is hopefully possible with the data at hand. The most frequent available data is the location and the join date of the reviewer. This information is quite important since a network can be created based on those attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEt all the reviwer info, location, join date, review count, upvotes, followers and following.\n",
    "def reviewerInfo(url):\n",
    "    username = url\n",
    "    full_url = f\"https://www.tripadvisor.com/Profile/{url}\"\n",
    "    driver.get(full_url)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Get Intro info, location and join date\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    try:\n",
    "        location = soup.find('span', class_ = \"_2VknwlEe _3J15flPT default\").text\n",
    "    except:\n",
    "        location = None\n",
    "\n",
    "    try:\n",
    "        joined = soup.find('span', class_ = \"_1CdMKu4t\").text\n",
    "    except:\n",
    "        joined = None\n",
    "\n",
    "\n",
    "    all_links = soup.find_all('div', class_ = '_1aVEDY08')\n",
    "    # link = driver.find_elements_by_xpath(\"//div[@class='nkw-3XeH']/div[1]/span[2]/a\")\n",
    "\n",
    "    # # Get the contributions info\n",
    "    nrContributions = int(str(all_links[0].text).split()[1])\n",
    "    if nrContributions > 0:\n",
    "        link = driver.find_elements_by_xpath(\"//div[@class='nkw-3XeH']/div[1]/span[2]/a\")\n",
    "        driver.execute_script(\"arguments[0].click();\", link[0])\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        nrReviews = int(str(soup.find('span', class_ = 'ui_icon pencil-paper _1LSVmZLi').parent.text).split()[0])\n",
    "        try:\n",
    "            nrUpvotes = int(str(soup.find('span', class_ ='ui_icon thumbs-up _1LSVmZLi _3zmXi7gU').parent.text).split()[0])\n",
    "        except:\n",
    "            nrUpvotes = 0\n",
    "        close = driver.find_elements_by_xpath(\"//div[@class='_2EFRp_bb _9Wi4Mpeb']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", close[0])\n",
    "    else:\n",
    "        nrReviews = 0\n",
    "        nrUpvotes = 0\n",
    "\n",
    "    # Get Followers\n",
    "    nrFollowers = int(str(all_links[1].text).split()[1])\n",
    "    if nrFollowers > 0:\n",
    "        link = driver.find_elements_by_xpath(\"//div[@class='nkw-3XeH']/div[2]/span[2]/a\")\n",
    "        driver.execute_script(\"arguments[0].click();\", link[0])\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        followers = [word.text for word in soup.find('div', class_='_1caczhWN').find_all('span', class_='gf69u3Nd')]\n",
    "        close = driver.find_elements_by_xpath(\"//div[@class='_2EFRp_bb _9Wi4Mpeb']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", close[0])\n",
    "    else:\n",
    "        followers = []\n",
    "\n",
    "    # Get all following\n",
    "    nrFollowing = int(str(all_links[2].text).split()[1])\n",
    "    if nrFollowing > 0:\n",
    "        link = driver.find_elements_by_xpath(\"//div[@class='nkw-3XeH']/div[3]/span[2]/a\")\n",
    "        driver.execute_script(\"arguments[0].click();\", link[0])\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        following = [word.text for word in soup.find('div', class_='_1caczhWN').find_all('span', class_='gf69u3Nd')]\n",
    "        close = driver.find_elements_by_xpath(\"//div[@class='_2EFRp_bb _9Wi4Mpeb']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", close[0])\n",
    "    else:\n",
    "        following = []\n",
    "\n",
    "    with open(pathtoReviewers, mode='a', encoding=\"utf-8\") as reviewer_data:\n",
    "        data_writer = csv.writer(reviewer_data, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "        data_writer.writerow([username, location, joined, nrContributions,nrReviews, nrUpvotes, nrFollowers, followers, nrFollowing,following])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialise files and get the data\n",
    "\n",
    "The data is gathered by firsly getting all urls then looping through them and scraping the information. Firtsly initilise the file and the relevant column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToReviews = \"reviews.csv\"\n",
    "restaurantInfo = \"restaurantInfo.csv\"\n",
    "pathAllRestaurants = \"AllRestaurants.txt\"\n",
    "\n",
    "with open(restaurantInfo, mode='a', encoding=\"utf-8\") as trip:\n",
    "    data_writer = csv.writer(trip, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "    data_writer.writerow(['storeName', 'storeAddress', 'avgRating', 'nrReviews', 'priceCategory','CousineType', 'Rank'])\n",
    "#webDriver init\n",
    "\n",
    "with open(pathToReviews, mode='a', encoding=\"utf-8\") as trip_data:\n",
    "    data_writer = csv.writer(trip_data, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "    data_writer.writerow(['storeName', 'reviewerUsername', 'ratingDate', 'reviewHeader','reviewText', 'rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the reviews\n",
    "Scrape all of the reviews and keep track of what urls are finished and who are left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"urls_left.txt\", \"rb\") as f:   # Unpickling\n",
    "    urls_left = pickle.load(f)\n",
    "\n",
    "# initialize the selenium driver    \n",
    "driver_path = f'../scraper/{os.getcwd()}/chromedriver'\n",
    "driver = webdriver.Chrome(driver_path)\n",
    "urls_left = urls.copy()\n",
    "\n",
    "# Do this in steps of 100 restaurants since the code takes multiple hours to run\n",
    "finished = []\n",
    "bad_url = []\n",
    "next_100 = urls_left[0:100]\n",
    "for url in next_100:\n",
    "    try:\n",
    "        get_reviews(url)\n",
    "        finished.append(url)\n",
    "    except:\n",
    "        bad_url.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the urls that are successfull. Take a look at those that failed and try to fix what was missing or caused the bad retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_urls = [urls_left[x].index for x in finished]\n",
    "\n",
    "for idx in finished_urls:\n",
    "    del urls_left[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting restaurant info\n",
    "\n",
    "Here we run a function the gets the website with a request, the html parser is then used from beautifulsoup and the needed tags are lacated and stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"AllRestaurants.txt\", \"rb\") as f:   # Unpickling\n",
    "    urls = pickle.load(f)\n",
    "\n",
    "bad_url = []\n",
    "for url in urls:\n",
    "    try:\n",
    "        scrapeRestaurantInfo(url)\n",
    "    except:\n",
    "        bad_url.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here bad_url contained restaurant with no reviews and thus deemed reduntant to our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting reviwer info\n",
    "Scraping the reviews info after the file has been created from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_path = f'{os.getcwd()}/chromedriver'\n",
    "driver = webdriver.Chrome(driver_path)\n",
    "\n",
    "with open(\"reviewers.txt\", \"r\") as f:\n",
    "    reviewers = f.readlines()\n",
    "\n",
    "for reviewer in reviewers:\n",
    "    reviewerInfo(reviewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging csv files\n",
    "\n",
    "In order to gather the data in a shorter time, multiple environments were run in paralell to gather the data simultaneously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "reader = csv.reader(open(\"Data/reviews1.csv\"))\n",
    "reader1 = csv.reader(open(\"Data/reviews2.csv\"))\n",
    "reader2 = csv.reader(open(\"Data/reviews22.csv\"))\n",
    "\n",
    "# Skip the first lines when combining\n",
    "next(reader1)\n",
    "next(reader2)\n",
    "\n",
    "f = open(\"Data/reviews.csv\", \"w\")\n",
    "writer = csv.writer(f)\n",
    "\n",
    "for row in reader:\n",
    "    writer.writerow(row)\n",
    "for row in reader1:\n",
    "    writer.writerow(row)\n",
    "for row in reader2:\n",
    "    writer.writerow(row)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"datasets\"></a>\n",
    "# 2. The datasets and data preparation\n",
    "\n",
    "As mentioned in the scraper section the tool used to to iteratevly go through every \"next\" page of a specific tripadvisor restaurant review and gather the wanted information. The restaurants info is also scraped seperately and lastly the reviwer's information is gathered after all the reviews have been collected.\n",
    "\n",
    "\n",
    "The data is saved into three files:\n",
    "1. **restaurantsInfo.csv**: Contains information about each resturant.\n",
    "2. **reviews.csv**: Contains the reviews for each restaurant.\n",
    "3. **reviewerInfo.csv**: Contains the reviews for each restaurant.\n",
    "\n",
    "The raw data is gathered from the csv files and cleaned and prepared for further analysis.\n",
    "\n",
    "Additionally some datasets were downloaded from the internet. These include shapefiles of Denmark with municipality division."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"restaurantInfo\"></a>\n",
    "## 2.1 restaurantInfo.csv\n",
    "\n",
    "**restaurantInfo.csv** containts information about each restaurant. This information are namely::\n",
    "* Restaurant name\n",
    "* Address\n",
    "* Average rating\n",
    "* Number of reviews\n",
    "* Price category\n",
    "* List of cousine types the restaurant offers\n",
    "* Rank\n",
    "\n",
    "Let's examine how the data looks by loading the **restaurantInfo.csv** into a pandas dataframe.\n",
    "\n",
    "Some restaurants contain several type of food and therefore the **CousineType** column contains a list for each restaurant and that is why we need to use converter to read it in correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>storeName</th>\n",
       "      <th>storeAddress</th>\n",
       "      <th>avgRating</th>\n",
       "      <th>nrReviews</th>\n",
       "      <th>priceCategory</th>\n",
       "      <th>CousineType</th>\n",
       "      <th>Rank</th>\n",
       "      <th>all_ranks</th>\n",
       "      <th>all_ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Krogs Fish Restaurant</td>\n",
       "      <td>Gammel Strand 38, Copenhagen 1202 Denmark</td>\n",
       "      <td>4.5</td>\n",
       "      <td>329</td>\n",
       "      <td>$$$$</td>\n",
       "      <td>['Seafood', 'European', 'Scandinavian']</td>\n",
       "      <td>#113</td>\n",
       "      <td>['#8 of 82 Seafood in Copenhagen', '#113 of 1,...</td>\n",
       "      <td>[('Food', 4.5), ('Service', 4.5), ('Value', 4.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maple Casual Dining</td>\n",
       "      <td>Vesterbrogade 24, Copenhagen 1620 Denmark</td>\n",
       "      <td>5.0</td>\n",
       "      <td>237</td>\n",
       "      <td>$$ - $$$</td>\n",
       "      <td>['International', 'European', 'Vegetarian Frie...</td>\n",
       "      <td>#1</td>\n",
       "      <td>['#1 of 95 International in Copenhagen', '#1 o...</td>\n",
       "      <td>[('Food', 5.0), ('Service', 5.0), ('Value', 4.5)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Keyser Social</td>\n",
       "      <td>Frederiksborggade 20d, Copenhagen 1360 Denmark</td>\n",
       "      <td>5.0</td>\n",
       "      <td>125</td>\n",
       "      <td>$$$$</td>\n",
       "      <td>['Asian', 'Thai', 'Vegetarian Friendly']</td>\n",
       "      <td>#2</td>\n",
       "      <td>['#1 of 226 Asian in Copenhagen', '#2 of 1,971...</td>\n",
       "      <td>[('Food', 5.0), ('Service', 5.0), ('Value', 5.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Restaurant Krebsegaarden</td>\n",
       "      <td>Studiestraede 17, Copenhagen 1455 Denmark</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1,403</td>\n",
       "      <td>$$$$</td>\n",
       "      <td>['European', 'Scandinavian', 'Danish']</td>\n",
       "      <td>#3</td>\n",
       "      <td>['#2 of 840 European in Copenhagen', '#3 of 1,...</td>\n",
       "      <td>[('Food', 5.0), ('Service', 5.0), ('Value', 4....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Olive Kitchen &amp; Bar</td>\n",
       "      <td>Noerregade 22, Copenhagen 1165 Denmark</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2,413</td>\n",
       "      <td>$$ - $$$</td>\n",
       "      <td>['International', 'European', 'Gluten Free Opt...</td>\n",
       "      <td>#4</td>\n",
       "      <td>['#2 of 95 International in Copenhagen', '#4 o...</td>\n",
       "      <td>[('Food', 5.0), ('Service', 5.0), ('Value', 4.5)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  storeName                                    storeAddress  \\\n",
       "0     Krogs Fish Restaurant       Gammel Strand 38, Copenhagen 1202 Denmark   \n",
       "1       Maple Casual Dining       Vesterbrogade 24, Copenhagen 1620 Denmark   \n",
       "2             Keyser Social  Frederiksborggade 20d, Copenhagen 1360 Denmark   \n",
       "3  Restaurant Krebsegaarden       Studiestraede 17, Copenhagen 1455 Denmark   \n",
       "4   The Olive Kitchen & Bar          Noerregade 22, Copenhagen 1165 Denmark   \n",
       "\n",
       "  avgRating nrReviews priceCategory  \\\n",
       "0       4.5       329          $$$$   \n",
       "1       5.0       237      $$ - $$$   \n",
       "2       5.0       125          $$$$   \n",
       "3       5.0     1,403          $$$$   \n",
       "4       5.0     2,413      $$ - $$$   \n",
       "\n",
       "                                         CousineType  Rank  \\\n",
       "0            ['Seafood', 'European', 'Scandinavian']  #113   \n",
       "1  ['International', 'European', 'Vegetarian Frie...    #1   \n",
       "2           ['Asian', 'Thai', 'Vegetarian Friendly']    #2   \n",
       "3             ['European', 'Scandinavian', 'Danish']    #3   \n",
       "4  ['International', 'European', 'Gluten Free Opt...    #4   \n",
       "\n",
       "                                           all_ranks  \\\n",
       "0  ['#8 of 82 Seafood in Copenhagen', '#113 of 1,...   \n",
       "1  ['#1 of 95 International in Copenhagen', '#1 o...   \n",
       "2  ['#1 of 226 Asian in Copenhagen', '#2 of 1,971...   \n",
       "3  ['#2 of 840 European in Copenhagen', '#3 of 1,...   \n",
       "4  ['#2 of 95 International in Copenhagen', '#4 o...   \n",
       "\n",
       "                                         all_ratings  \n",
       "0  [('Food', 4.5), ('Service', 4.5), ('Value', 4.0)]  \n",
       "1  [('Food', 5.0), ('Service', 5.0), ('Value', 4.5)]  \n",
       "2  [('Food', 5.0), ('Service', 5.0), ('Value', 5.0)]  \n",
       "3  [('Food', 5.0), ('Service', 5.0), ('Value', 4....  \n",
       "4  [('Food', 5.0), ('Service', 5.0), ('Value', 4.5)]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurants = pd.read_csv('Data/restaurantInfo.csv')\n",
    "restaurants.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **priceCategory** column appears to be displaying in a weird way or not showing all the data. However when a single row is printed, the correct format of the column can be seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "storeName                                    Krogs Fish Restaurant\n",
       "storeAddress             Gammel Strand 38, Copenhagen 1202 Denmark\n",
       "avgRating                                                      4.5\n",
       "nrReviews                                                      329\n",
       "priceCategory                                                 $$$$\n",
       "CousineType                ['Seafood', 'European', 'Scandinavian']\n",
       "Rank                                                          #113\n",
       "all_ranks        ['#8 of 82 Seafood in Copenhagen', '#113 of 1,...\n",
       "all_ratings      [('Food', 4.5), ('Service', 4.5), ('Value', 4.0)]\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurants.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning üßπ \n",
    "A clean-up needs to be performed before the dataset is used for further analysis. Since '$$' is a keyword in Matplotlib, the **priceCategory** column has to be mapped to something. A mapping to integers was created, since they are a nice way to represent the data.\n",
    "\n",
    "Additionally the **Rank** column is modified, in such a way that the '#' symbol is removed so the column can be converted from a string to integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'Rank'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9884da20aa66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Remove the '#' in the front of the Rank column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrestaurants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestaurants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-9884da20aa66>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Remove the '#' in the front of the Rank column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrestaurants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestaurants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'Rank'"
     ]
    }
   ],
   "source": [
    "# Since the Pandas profiler can no display string with '$$' it is necessary to map the price categories differently\n",
    "restaurants.priceCategory = restaurants.priceCategory.map({'$': 1, '$ - $$': 1.5, '$$': 2, '$$ - $$$': 2.5, '$$$': 3, '$$$ - $$$$': 3.5, '$$$$': 4, '$$$$ - $$$$$': 4.5, '$$$$$': 5})\n",
    "\n",
    "# Remove the '#' in the front of the Rank column\n",
    "restaurants.Rank = restaurants.Rank.apply(lambda x: x.replace('#','')).apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally it would be benefitial to get the latitude and longitude coordinates of the restaurant address, so that it can easily be plotted on a map and used for further analysis. This can be achieved by using the address as a query string and calling the Open Street maps. Since some of the addresses are in a weird format, the name of the restaurant will first be used in the search query (with 'denmark' added at the end to clarify the search). If the location information can be found from the restaurant name, that will be used. If the location information can not be found from the name then the address will be used to generate the latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all the rows of the dataset and gather the lat and lon info into vectors\n",
    "# First we use the lat/lon info found from the restaurant name since that could be considered more accurate \n",
    "# since users have to label the restaurant on a map. Otherwise the info from the address is used.\n",
    "# Finally we collect the display name, to be able to extract the municipality information\n",
    "\n",
    "# Create vectors to store information\n",
    "lats = []\n",
    "lons = []\n",
    "displayNames = []\n",
    "\n",
    "for idx, row in restaurants.iterrows():\n",
    "    \n",
    "    lat = None\n",
    "    lon = None\n",
    "    displayName = None\n",
    "    address = row.storeAddress\n",
    "    name = row.storeName\n",
    "    url1 = 'https://nominatim.openstreetmap.org/search/' + urllib.parse.quote(name + \" denmark\") +'?format=json'\n",
    "    url2 = 'https://nominatim.openstreetmap.org/search/' + urllib.parse.quote(address) +'?format=json'\n",
    "\n",
    "    # Make the first request based on the name\n",
    "    response1 = requests.get(url1).json()\n",
    "    \n",
    "    if response1:\n",
    "        lat = response1[0][\"lat\"]\n",
    "        lon = response1[0][\"lon\"]\n",
    "        displayName = response1[0][\"display_name\"]\n",
    "    else:\n",
    "        # make the second request based on the address\n",
    "        response2 = requests.get(url2).json()\n",
    "        if response2:\n",
    "            lat = response2[0][\"lat\"]\n",
    "            lon = response2[0][\"lon\"]\n",
    "            displayName = response2[0][\"display_name\"]\n",
    "    \n",
    "    # Append the info we gathered into the vectors\n",
    "    lats.append(lat)\n",
    "    lons.append(lon)\n",
    "    displayNames.append(displayName)\n",
    "    \n",
    "# Add the vectors to the dataset\n",
    "restaurants['lat'] = lats\n",
    "restaurants['lon'] = lons\n",
    "restaurants['location'] = displayNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this data preparation the information about a single restaurant appears like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reviews\"></a>\n",
    "## 2.2 reviews.csv\n",
    "\n",
    "**reviews.csv** containts information about each review for every restaurant. Namely:\n",
    "* Restaurant name\n",
    "* Reviewer's username\n",
    "* Date of rating\n",
    "* Header of review\n",
    "* Review text\n",
    "* User's rating\n",
    "\n",
    "Let's examine how the data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('reviews.csv')\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above dataset appears ready for use, however the **ratingDate** is presented as a string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviews.iloc[0].ratingDate)\n",
    "print(type(reviews.iloc[0].ratingDate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further analysis, it will be benefitial to have converted this column into a timestamp object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['ratingDate'] = reviews['ratingDate'].apply(lambda x: datetime.strptime(x, '%B %d, %Y'))\n",
    "print(reviews.iloc[0].ratingDate)\n",
    "print(type(reviews.iloc[0].ratingDate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"shapefiles\"></a>\n",
    "## 2.3 Shapefiles\n",
    "Shapefiles for the whole of Denmark were downloaded from [here](https://www.diva-gis.org/datadown).\n",
    "The files include information about the municipalities and their division within Denmark.\n",
    "These can be used later on when performing analysis that require information about different areas of Denmark.\n",
    "\n",
    "The first step is to load and examine the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the shapefiles and \n",
    "shp = 'DNK_adm/DNK_adm2.shp'\n",
    "gdf = gpd.read_file(shp)\n",
    "\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any analysis performed will be focused on the capital region, so the shapefile is filtered on *Hovedstaden* and some of the included islands are skipped so the plot will be as clear as possible. Each municipality will be plotted in a seperate color based on their numerical ID in the shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter on the areas we want displayed\n",
    "hovedstaden = gdf[(gdf.ID_1==1) & (gdf.NAME_2 != 'Bornholm') & (gdf.NAME_2 != 'Christians√∏') & (gdf.NAME_2 != 'Halsn√¶s')]\n",
    "\n",
    "# Plot the shapefiles\n",
    "fig, ax = plt.subplots(1, figsize=(14, 8));\n",
    "hovedstaden.plot(column='ID_2', cmap='tab20b', linewidth=0.8, ax=ax, edgecolor='black', legend=True);\n",
    "ax.axis('off');\n",
    "ax.set_title('Denmark', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"descStats\"></a>\n",
    "# 3. Descriptive stats\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"restaurants\"></a>\n",
    "## 3.1. Restaurants\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a profile for the dataset and display it\n",
    "restaurant_profile = ProfileReport(restaurants, title=\"Restaurants Info dataset\", html={'style': {'full_width': True}});\n",
    "restaurant_profile.to_notebook_iframe();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**ADD DISCUSSION ON FINAL FINDINGS**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the profiler it can be seen that it does not handle a column containing a list. Therefore we do an additional check for the **CousineType** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CousineTypeFlat = [y for x in restaurants.CousineType for y in x]\n",
    "\n",
    "# https://stackoverflow.com/questions/49017002/bar-plot-based-on-list-of-string-values\n",
    "keys, counts = np.unique(CousineTypeFlat, return_counts=True)\n",
    "\n",
    "counts, keys = zip(*sorted(zip(counts, keys), reverse=True))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(keys, counts)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://georgetsilva.github.io/posts/mapping-points-with-folium/\n",
    "locations = restaurants[['lat', 'lon']]\n",
    "locationlist = locations.values.tolist()\n",
    "\n",
    "map = folium.Map(location=[55.7, 12.6], zoom_start=12)\n",
    "for point in range(0, len(locationlist)):\n",
    "    folium.Marker(locationlist[point], popup=restaurants['storeName'][point]).add_to(map)\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reviews\"></a>\n",
    "## 3.2 Reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a profile for the dataset and display it\n",
    "reviews_profile = ProfileReport(reviews, title=\"Reviews dataset\", html={'style': {'full_width': True}});\n",
    "reviews_profile.to_notebook_iframe();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='businessquestions'></a>\n",
    "# 4. Business Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The business question for the project are the following. \n",
    "\n",
    "<font color='red'>**ADD any missing question kve√∞ja Hlynur**</font>\n",
    "\n",
    "1. Can we predict up and coming neigborhoods based on restaurant rating or newly opened restaurants ?\n",
    "2. Is the advertised cousine type of the restaurant represent what the reviewers really like ?\n",
    "3. Showing the trend of restaurant and how the movement based on rating and restaurant availability has changed over time \n",
    "4. Provide insight into what makes a good and bad review, what are the keywoards and can we filter out to get a report for the restaurant improvement points ? \n",
    "5. Network analysis based on the connection between reviewers or restaurants\n",
    "6. Where should you place your restaurant in the city based on the surrounding restaurant types and causines. Is there space for a new pizza place or are they simply to many ? Are there only low rated pizza places in the neighbourhood where your ambition will thrive ? \n",
    "7. Recommendation system based on similar reviewer's interest or similar restaurant characteristics\n",
    "8. Create a machine learning model to predict a review rating based on the review text.\n",
    "\n",
    "<font color='red'>**ADD more detailed descirption of each business question**</font>\n",
    "\n",
    "Hlynur/Krist√≠n - 1, 2, 3, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
